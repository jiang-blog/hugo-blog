---
tags: []
categories: ["work", "openvino_pot"]
description:
summary:
draft: false
isCJKLanguage: true
lastmod: "2022-06-10"
title: 1-1-量化模型
date: "2022-05-26"
---

# 1-1-量化模型

## 简介

本文描述了如何在不控制精度的情况下，使用默认量化方法对未注释的数据集进行模型量化处理。为了使用这种方法，你需要使用训练后优化工具（POT）的API创建一个Python_脚本，并实现数据准备逻辑和量化管道。如果你不熟悉Python_，你可以试试POT的[命令行界面](1-5_命令行界面.md)，它被设计为从OpenVINO [Model Zoo](https://github.com/openvinotoolkit/open_model_zoo)量化模型。下图显示了用POT API实现的量化脚本的常见工作流程：

![](https://raw.githubusercontent.com/jiang-blog/BlogImgs/picgo/imgs/Post-Training%20Optimization%20Tool%20API.png)

该脚本应包括三个基本步骤：

1. 准备好数据和数据集界面
2. 选择量化参数
3. 定义并运行量化过程

## 准备数据和数据集接口

在大多数情况下，只需要实现`openvino.tools.pot.DataLoader`接口，该接口允许从数据集获取数据并应用特定模型的预处理，提供索引访问。任何实现都应该覆盖以下方法。

- `__len__()`, 返回数据集的大小。

- `__getitem__()`, 提供在 0 到 ` len(self)  `范围内通过索引访问数据。它也可以封装特定模型预处理的逻辑。该方法应该以如下格式返回数据：
  - `(data, annotation)`

`data`可以是`numpy.array`对象或字典，其中`key`是模型输入的名称，`value`是与此输入相对应的`numpy.array`。由于`annotation`不被默认量化方法使用，在这种情况下，这个对象可以是`None`。

你可以通过`openvino.tools.pot.DataLoader`接口来包装框架的数据加载类，这通常是很直接的。例如，`torch.utils.data.Dataset`有一个与`openvino.tools.pot.DataLoader`类似的接口，因此它的TorchVision实现可以很容易地被POT API包裹起来。

> **注意事项**
> 特定于模型的预处理，例如，平均/尺度归一化可以在转换步骤中使用模型优化器组件嵌入到模型中。在实现`DataLoader`接口的过程中应该考虑到这一点，以避免 "双重 "规范化，这可能导致优化后的精度损失。

下面的代码例子为图像的使用案例定义了`DataLoader`：

```python
import os

import numpy as np
import cv2 as cv

from openvino.tools.pot import DataLoader

class ImageLoader(DataLoader):
    """ Loads images from a folder """
    def __init__(self, dataset_path):
        # Use OpenCV to gather image files
        # Collect names of image files
        self._files = []
        all_files_in_dir = os.listdir(dataset_path)
        for name in all_files_in_dir:
            file = os.path.join(dataset_path, name)
            if cv.haveImageReader(file):
                self._files.append(file)

        # Define shape of the model
        self._shape = (224,224)

    def __len__(self):
        """ Returns the length of the dataset """
        return len(self._files)

    def __getitem__(self, index):
        """ Returns image data by index in the NCHW layout
        Note: model-specific preprocessing is omitted, consider adding it here
        """
        if index >= len(self):
            raise IndexError("Index out of dataset size")

        image = cv.imread(self._files[index]) # read image with OpenCV
        image = cv.resize(image, self._shape) # resize to a target input size
        image = np.expand_dims(image, 0)  # add batch dimension
        image = image.transpose(0, 3, 1, 2)  # convert to NCHW layout
        return image, None   # annotation is set to None
```

## 选择量化参数

默认的量化算法有必须的和可选的参数，它们被定义为一个字典。

```python
{
    "name": "DefaultQuantization"。
    "params": {
        "target_device": "ANY",
        "stat_subset_size": 300
    },
}
```

`"target_device"` - 目前，只有两个选项可用。"ANY"（或 "CPU"）--为CPU、GPU或VPU量化模型，"GNA"--用于在GNA上进行推理。

`"stat_subset_size"` - 计算量化所用的激活统计的数据子集的大小。如果没有指定参数，则使用整个数据集。我们建议使用不少于300个样本。

默认量化方法的完整规范可在本文件中找到。

## 运行量化

POT API提供了自己的方法来加载和保存来自OpenVINO中间表征的模型对象：`load_model`和`save_model`。它还有一个Pipeline的概念，依次对模型应用指定的优化方法。create_pipeine方法用于实例化一个Pipeline对象。下面的一个代码例子显示了一个基本的量化工作流程：

```python
from openvino.tools.pot import IEEngine
from openvino.tools.pot load_model, save_model
from openvino.tools.pot import compress_model_weights
from openvino.tools.pot import create_pipeline

# Model config specifies the model name and paths to model .xml and .bin file
model_config =
{
    "model_name": "model",
    "model": path_to_xml,
    "weights": path_to_bin,
}

# Engine config
engine_config = {"device": "CPU"}

algorithms = [
    {
        "name": "DefaultQuantization",
        "params": {
            "target_device": "ANY",
            "stat_subset_size": 300
        },
    }
]

# Step 1: Implement and create user's data loader
data_loader = ImageLoader("<path_to_images>")

# Step 2: Load model
model = load_model(model_config=model_config)

# Step 3: Initialize the engine for metric calculation and statistics collection.
engine = IEEngine(config=engine_config, data_loader=data_loader)

# Step 4: Create a pipeline of compression algorithms and run it.
pipeline = create_pipeline(algorithms, engine)
compressed_model = pipeline.run(model=model)

# Step 5 (Optional): Compress model weights to quantized precision
#                     to reduce the size of the final .bin file.
compress_model_weights(compressed_model)

# Step 6: Save the compressed model to the desired path.
# Set save_path to the directory where the model should be saved
compressed_model_paths = save_model(
    model=compressed_model,
    save_path="optimized_model",
    model_name="optimized_model",
)
```

脚本的输出是量化后的模型，可以和原始全精度模型一样用于推理。

如果应用默认量化方法后精度下降很高，建议尝试[量化最佳实践](1-3_量化最佳实践.md)文件中的提示或使用[精度感知的量化方法](1-2_精度感知量化模型.md)。

## 对级联模型进行量化

在某些情况下，当优化模型是一个级联模型，即由几个子模型组成，例如MT-CNN，你将需要实现一个复杂的推理管道，可以适当处理不同的子模型和它们之间的数据流。POT API为此提供了一个`Engine`接口，允许定制推理逻辑。然而，我们建议继承`IEEngine`辅助类，它已经包含了基于OpenVINO Python API进行推理的所有逻辑。请看下面的[例子](https://docs.openvino.ai/latest/pot_example_face_detection_README.html#)。

## 实例

- 教程：
  - 图像分类模型的量化
  - 模型动物园的物体检测模型的量化
  - 医学数据分割模型的量化
  - 文本分类中BERT的量化

- 样本：
  - 三维分割模型的量化
  - 人脸检测模型的量化
  - 用于GNA设备的语音模型的量化

